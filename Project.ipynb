{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "434786a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sounddevice as sd\n",
    "import librosa\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow import keras\n",
    "from langchain.chat_models import ChatGooglePalm\n",
    "from langchain.prompts import HumanMessagePromptTemplate\n",
    "from langchain.schema.messages import SystemMessage\n",
    "from langchain.embeddings import GooglePalmEmbeddings\n",
    "from langchain.llms import GooglePalm\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "import google.generativeai\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "import os\n",
    "from langchain.prompts import PromptTemplate, StringPromptTemplate\n",
    "import speech_recognition as sr\n",
    "import sounddevice as sd\n",
    "import scipy.io.wavfile as wav\n",
    "import numpy as np\n",
    "import pyaudio\n",
    "import wave\n",
    "import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "485557c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to your audio dataset\n",
    "data_dir = 'E:/capstone/archive/Raw_data/wavfiles/'\n",
    "\n",
    "# Function to extract features from audio files\n",
    "def extract_features(file_path):\n",
    "    audio, sr = librosa.load(file_path, res_type='kaiser_fast')\n",
    "    mfccs = np.mean(librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=13), axis=1)\n",
    "    chroma = np.mean(librosa.feature.chroma_stft(y=audio, sr=sr), axis=1)\n",
    "    mel = np.mean(librosa.feature.melspectrogram(y=audio, sr=sr), axis=1)\n",
    "    spectral_contrast = np.mean(librosa.feature.spectral_contrast(y=audio, sr=sr), axis=1)\n",
    "    return np.concatenate((mfccs, chroma, mel, spectral_contrast))\n",
    "\n",
    "# Create an empty list to store the features and labels\n",
    "features = []\n",
    "labels = []\n",
    "\n",
    "# Define a mapping from original labels to new emotions\n",
    "label_mapping = {\n",
    "    'encouraging': 'emotion_1',\n",
    "    'assertive': 'emotion_2',\n",
    "    'apologetic': 'emotion_3',\n",
    "    'sad': 'emotion_4',\n",
    "    'excited': 'emotion_5',\n",
    "    'angry': 'emotion_6',\n",
    "    'anxious': 'emotion_7',\n",
    "    'happy': 'emotion_8',\n",
    "    'neutral': 'emotion_9',\n",
    "    'concerned': 'emotion_10'\n",
    "}\n",
    "\n",
    "# Iterate through the audio files and map the original labels to new emotions\n",
    "for filename in os.listdir(data_dir):\n",
    "    if filename.endswith('.wav'):\n",
    "        emotion = filename.split('_')[1]\n",
    "        # Map the emotion to the new label\n",
    "        new_emotion = label_mapping[emotion]\n",
    "        file_path = os.path.join(data_dir, filename)\n",
    "        features.append(extract_features(file_path))\n",
    "        labels.append(new_emotion)\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "labels = label_encoder.fit_transform(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d13f9677",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(file_path):\n",
    "    audio, sr = librosa.load(file_path, res_type='kaiser_fast')\n",
    "    mfccs = np.mean(librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=13), axis=1)\n",
    "    chroma = np.mean(librosa.feature.chroma_stft(y=audio, sr=sr), axis=1)\n",
    "    mel = np.mean(librosa.feature.melspectrogram(y=audio, sr=sr), axis=1)\n",
    "    spectral_contrast = np.mean(librosa.feature.spectral_contrast(y=audio, sr=sr), axis=1)\n",
    "    return np.concatenate((mfccs, chroma, mel, spectral_contrast))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3e227a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def record_audio(output_dir, filename, duration=5):\n",
    "    audio = pyaudio.PyAudio()\n",
    "\n",
    "    # Create an audio stream\n",
    "    stream = audio.open(format=pyaudio.paInt16, channels=1, rate=44100, input=True, frames_per_buffer=1024)\n",
    "\n",
    "    print(\"Recording...\")\n",
    "\n",
    "    frames = []\n",
    "    for i in range(0, int(44100 / 1024 * duration)):\n",
    "        data = stream.read(1024)\n",
    "        frames.append(data)\n",
    "\n",
    "    print(\"Finished recording.\")\n",
    "    \n",
    "    stream.stop_stream()\n",
    "    stream.close()\n",
    "    audio.terminate()\n",
    "\n",
    "    # Save the recorded audio as a .wav file\n",
    "    file_path = os.path.join(output_dir, filename)\n",
    "    with wave.open(file_path, 'wb') as wf:\n",
    "        wf.setnchannels(1)\n",
    "        wf.setsampwidth(audio.get_sample_size(pyaudio.paInt16))\n",
    "        wf.setframerate(44100)\n",
    "        wf.writeframes(b''.join(frames))\n",
    "    \n",
    "    return file_path\n",
    "\n",
    "def convert_audio_text(audio_path):\n",
    "    recognizer = sr.Recognizer()\n",
    "\n",
    "    with sr.AudioFile(audio_path) as source:\n",
    "        audio_data = recognizer.record(source)\n",
    "\n",
    "    try:\n",
    "        text = recognizer.recognize_google(audio_data)  # You can use other recognition engines too\n",
    "        return text\n",
    "    except sr.UnknownValueError:\n",
    "        return \"Google Web Speech API could not understand audio\"\n",
    "    except sr.RequestError as e:\n",
    "        return f\"Could not request results from Google Web Speech API; {e}\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "450b00d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"GOOGLE_API_KEY\"] = 'AIzaSyDUnyNoM6LzupQRCYpeQg5aXdGumekVbsE'\n",
    "llm = GooglePalm()\n",
    "llm.temperature = 0.3\n",
    "model = keras.models.load_model('E:/capstone/ml_paper/Saved_Model/speech_emotion_model.h5')\n",
    "\n",
    "memory = ConversationBufferMemory()\n",
    "template = \"\"\"\\Please act as an interviewer and ask a series of questions to gather information, \n",
    "conduct interview based on the context below, dont hallucinate./\n",
    "\n",
    "Rules:Please act as an interviewer for the below and follow the rules.Rules/\n",
    "        1-Dont complete the conversation by your-self.\n",
    "        2-dont ask irrlevant questions.\n",
    "        3-Dont give the candidate's response while giving output.\n",
    "        3-Dont repeat questions.\n",
    "        4-ask some technical questions.\n",
    "        \n",
    "```\n",
    "{history}\n",
    "```\n",
    "\n",
    "human: {input}\n",
    "\n",
    "Ai: \"\"\"\n",
    "\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"history\",\"input\"],\n",
    "    template=template\n",
    ")\n",
    "\n",
    "conversation_chain = LLMChain(llm=llm,prompt=prompt_template\n",
    "                              ,memory=memory)\n",
    "\n",
    "user =[]\n",
    "emotion = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8097e3b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording...\n",
      "Finished recording.\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "hello\n",
      "Hello, thanks for taking the time to meet with me today. Can you please tell me a little bit about your background?\n",
      "Recording...\n",
      "Finished recording.\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "Google Web Speech API could not understand audio\n",
      "Can you try rephrasing that?\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    for i in range(2):\n",
    "  \n",
    "        output_dir = \"output\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        audio_file = record_audio(output_dir, \"recorded_audio.wav\")\n",
    "        text = convert_audio_text(audio_file)\n",
    "        new_features = extract_features(audio_file)\n",
    "\n",
    "        # Reshape the extracted features (assuming your model expects a batch of samples)\n",
    "        new_features = np.reshape(new_features, (1, new_features.shape[0]))\n",
    "        # Make a prediction\n",
    "        predicted_label_index = np.argmax(model.predict(new_features))\n",
    "        predicted_label = label_encoder.inverse_transform([predicted_label_index])[0]\n",
    "        output = conversation_chain(text)\n",
    "        print(text)\n",
    "        print(output[\"text\"])\n",
    "        \n",
    "        user[i] = user.append(text)\n",
    "        emotion[i] = emotion.append(predicted_label)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "61984cf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thank you for joining with me today.You feedback will be provided shortly\n",
      "{None: None, 'hi': 'angry', 'of course my name is Kishore and I am from Chennai': 'angry'}\n"
     ]
    }
   ],
   "source": [
    "# Map numeric labels to emotions\n",
    "for j in range(len(emotion)):\n",
    "    if emotion[j]==\"emotion_1\":\n",
    "        \n",
    "        emotion[j]=\"encouraging\"\n",
    "        \n",
    "    if emotion[j]==\"emotion_2\":    \n",
    "        \n",
    "        emotion[j]='assertive'\n",
    "        \n",
    "    if emotion[j]==\"emotion_3\":\n",
    "        \n",
    "        emotion[j]='apologetic'\n",
    "        \n",
    "    if emotion[j]==\"emotion_4\":\n",
    "        \n",
    "        emotion[j]='sad'\n",
    "        \n",
    "    if emotion[j]==\"emotion_5\":\n",
    "        \n",
    "        emotion[j]='excited'\n",
    "        \n",
    "    if emotion[j]==\"emotion_6\":\n",
    "        \n",
    "        emotion[j]=\"angry\"\n",
    "        \n",
    "    if emotion[j]==\"emotion_7\":\n",
    "        \n",
    "        emotion[j]='anxious'\n",
    "        \n",
    "    if emotion[j]==\"emotion_8\":\n",
    "        \n",
    "        emotion[j]='happy'\n",
    "        \n",
    "    if emotion[j]==\"emotion_9\":\n",
    "        \n",
    "        emotion[j]='neutral'\n",
    "    \n",
    "    if emotion[j]==\"emotion_10\":\n",
    "        \n",
    "        emotion[j]='concerned'\n",
    "               \n",
    "print(\"Thank you for joining with me today.You feedback will be provided shortly\")\n",
    "\n",
    "# Combine the user messages and mapped emotions into a dictionary\n",
    "response = dict(zip(user, emotion))\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3240d053",
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_responses = '\\n'.join([f\"{k}: {v}\" for k, v in response.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "24327c62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The interview response is very short and doesn't provide much information. The interviewee also seems to be angry, which is not a good sign. Here are some constructive feedback:\n",
      "\n",
      "* **Be more conversational.** The interviewee should try to be more conversational and provide more details about themselves. This will help the interviewer get to know them better.\n",
      "* **Be positive.** The interviewee should try to be more positive and avoid coming across as angry. This will make a better impression on the interviewer.\n",
      "* **Be prepared.** The interviewee should be prepared for the interview and have answers to common interview questions. This will help them to make a good impression and increase their chances of getting the job.\n"
     ]
    }
   ],
   "source": [
    "import google.generativeai as palm\n",
    "palm.configure(api_key=\"AIzaSyDeYshE-LEcHkTca2FjXWlxCXbMjIT72Dc\")\n",
    "\n",
    "defaults = {\n",
    "  'model': 'models/text-bison-001',\n",
    "  'temperature': 0.55,\n",
    "  'candidate_count': 1,\n",
    "  'top_k': 40,\n",
    "  'top_p': 0.95,\n",
    "  'max_output_tokens': 1024,\n",
    "  'stop_sequences': [],\n",
    "  'safety_settings': [],\n",
    "}\n",
    "prompt = f\"\"\"Evaluate the following interview response and the associated emotions, and provide constructive feedback:\n",
    "\n",
    "Interview Response:\n",
    "   \n",
    "```\n",
    "{formatted_responses}\n",
    "```\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "response = palm.generate_text(**defaults,prompt=prompt)\n",
    "print(response.result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757a2eac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
